{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies nécessaires\n",
    "import dotenv # Pour lire nos variables environnements avec nos APIs\n",
    "\n",
    "# On importe quelques librairies de manipulation de données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# On importe les modules nécessaires de LangChain\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lit nos variables environnments avec nos clés APIs\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_new_tokens\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "        {\n",
      "            \"lieu\": \"Lyon\",\n",
      "            \"date\": \"2024-03-21\",\n",
      "            \"informations_supplementaires\": [\n",
      "                {\n",
      "                    \"question\": \"Est-ce qu'il va pleuvoir à Lyon jeudi prochain ?\",\n",
      "                    \"reponse\": \"Non, la réponse à votre question dépendra des prévisions météorologiques précises du jour concerné.\"\n",
      "                }\n",
      "            ]\n",
      "        }\n"
     ]
    }
   ],
   "source": [
    "template = \"\"\"[INST]\n",
    "        Tu dois extraire des informations de la phrase données. N'invente pas, et extrais dans un JSON le lieu et la date.\n",
    "        Aujourd'hui, nous sommes le 13/03/2024.\n",
    "        Renvoit un JSON de la forme avec le \"lieu\" et la \"date\", ainsi que les informations additionnelles dans une liste\n",
    "        ----- \n",
    "\n",
    "        Voici la requête :\n",
    "            {query}\n",
    "\n",
    "            [/INST]\n",
    "        JSON:\n",
    "\"\"\"\n",
    "\n",
    "query = \"Donne le temps à Lyon jeudi prochain, est-ce qu'il va pleuvoir\"\n",
    "\n",
    "# On instancie notre template de prompt où l'on indique que nos deux variables entrantes sont le contexte (documents) et la requête (question)\n",
    "promp_rag = PromptTemplate(input_variables=[\"query\"], template=template)\n",
    "chain = LLMChain(prompt=promp_rag, llm=llm,verbose=False)\n",
    "response = chain.invoke({\"query\": query})\n",
    "answer = response[\"text\"].split(\"JSON:\")[1]\n",
    "\n",
    "# On le place dans une variable pour indiquer que ce sera le prompt de notre retriever\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
