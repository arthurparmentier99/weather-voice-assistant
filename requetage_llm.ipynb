{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On importe les librairies nécessaires\n",
    "import dotenv # Pour lire nos variables environnements avec nos APIs\n",
    "\n",
    "# On importe quelques librairies de manipulation de données\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "# On importe les modules nécessaires de LangChain\n",
    "from langchain.chains import RetrievalQA, LLMChain\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.llms import HuggingFaceHub, HuggingFaceEndpoint\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import datetime\n",
    "import json\n",
    "tod_date = datetime.date.today()\n",
    "tod_hour = datetime.datetime.now()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On lit nos variables environnments avec nos clés APIs\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo_id = \"mistralai/Mixtral-8x7B-Instruct-v0.1\"\n",
    "llm = HuggingFaceHub(repo_id=repo_id, model_kwargs={\"temperature\": 0.1, \"max_new_tokens\":500})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_after_last_brace(text):\n",
    "    # Index de la dernière accolade fermante\n",
    "    last_brace_index = -1\n",
    "\n",
    "    for i, char in enumerate(text):\n",
    "        if char == '}':\n",
    "            last_brace_index = i\n",
    "    json_str = text[:last_brace_index+1]\n",
    "    return json.loads(json_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ville': 'Lyon', 'date': '2024/03/14', 'heure': '18'}\n"
     ]
    }
   ],
   "source": [
    "temp1 = \"\"\"[INST]\n",
    "        Tu dois extraire des informations de la phrase données.\n",
    "\n",
    "        N'invente pas, et extrais dans un JSON valide la VILLE et la DATE et l'HEURE. Si tu ne sait pas, met 'None'.\n",
    "        l'HEURE doit etre une heure valide.\n",
    "        Aujourd'hui, nous sommes le {0} à {1}h.\n",
    "\n",
    "        Le JSON doit avoir ce format:\n",
    "        (\n",
    "        \"ville\":\"ville\",\n",
    "        \"date\":\"YYYY/MM/DD\",\n",
    "        \"heure\":\"HH\"\n",
    "        )\n",
    "\n",
    "        ----- \n",
    "\"\"\".format(tod_date.strftime('%Y/%m/%d'),tod_hour.strftime('%H'))\n",
    "temp2 = \"\"\"\n",
    "        Voici la requête :\n",
    "            {query}\n",
    "\n",
    "            [/INST]\n",
    "        JSON:\n",
    "\"\"\"\n",
    "\n",
    "templ = temp1 + temp2\n",
    "\n",
    "query = \"Quelle meteo pour demain soir à Lyon?\"\n",
    "\n",
    "# On instancie notre template de prompt où l'on indique que nos deux variables entrantes sont le contexte (documents) et la requête (question)\n",
    "promp_rag = PromptTemplate(input_variables=[\"query\"], template=templ)\n",
    "chain = LLMChain(prompt=promp_rag, llm=llm,verbose=False)\n",
    "response = chain.invoke({\"query\": query})\n",
    "answer = response[\"text\"].split(\"JSON:\")[1]\n",
    "json_fin = remove_after_last_brace(answer)\n",
    "\n",
    "# On le place dans une variable pour indiquer que ce sera le prompt de notre retriever\n",
    "print(json_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_json(json_object,date,hour):\n",
    "    tod_h = hour.strftime('%H')\n",
    "    new_json = json_object\n",
    "    if list(json_fin.values()) == ['None','None','None']:\n",
    "        return \"Need to re-ask\"\n",
    "    if json_object[\"ville\"] == 'None':\n",
    "        new_json[\"ville\"] = \"Lyon\"\n",
    "    \n",
    "    day_plus_five = date + datetime.timedelta(days=5)\n",
    "    if json_object[\"heure\"] < tod_h : \n",
    "        json_object[\"heure\"] = tod_h + 1\n",
    "    if (json_object[\"date\"] > day_plus_five.strftime('%Y/%m/%d')) & (json_object[\"date\"] != 'None'):        \n",
    "        new_json[\"date\"] = day_plus_five.strftime('%Y/%m/%d')\n",
    "    \n",
    "    return new_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ville': 'Lyon', 'date': '2024/03/14', 'heure': '18'}"
      ]
     },
     "execution_count": 228,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_json = clean_json(json_fin,tod_date,tod_hour)\n",
    "new_json"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "td_llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
